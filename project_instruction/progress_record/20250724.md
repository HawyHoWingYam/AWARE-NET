# Stage 2 Implementation Plan - Precision Analyzer (2025-07-24)

## ðŸŽ¯ Stage 2 Overall Objective
Build the "heart" of the cascade system - two complementary precision analysis models that form heterogeneous expert analyzers for complex samples that cannot be determined by Stage 1.

## ðŸ“Š Current Project Status
- âœ… **Stage 0 Completed**: Environment setup, data preprocessing, multi-threading GPU acceleration
- âœ… **Stage 1 Completed**: MobileNetV4 fast filter with excellent performance (AUC: 0.9733, F1: 0.9706)
- âœ… **Documentation Updated**: All project documentation aligned with current implementation
- ðŸ”„ **Stage 2 Starting**: Precision analyzer implementation begins

## ðŸš€ Stage 2 Core Architecture Strategy

### **Heterogeneous Complementarity Design**
The Stage 2 system employs two expert models with different architectural strengths:

1. **EfficientNetV2-B3 (CNN Expert)**:
   - **Specialization**: Local texture anomaly detection
   - **Strengths**: Frequency domain artifacts, edge inconsistencies, compression artifacts
   - **Role**: "Local Feature Expert" for detailed texture analysis

2. **GenConViT (Generative Hybrid Expert)**:
   - **Specialization**: Global semantic inconsistency + generative artifact detection
   - **Strengths**: ConvNeXt-Swin hybrid attention, autoencoder reconstruction, VAE latent analysis
   - **Role**: "Generative-Discriminative Expert" combining reconstruction and attention mechanisms

### **Integration Strategy**
- **Embedding Extraction**: Extract deep feature vectors before classification heads
- **Feature Alignment**: Project different dimensions to unified 256-D space
- **Stacking Preparation**: Prepare heterogeneous features for Stage 3 meta-model

## ðŸ“‹ Stage 2 Implementation Tasks

### Task 2.1: EfficientNetV2-B3 Model Training â³
**Script**: `src/stage2/train_stage2_effnet.py`
**Timeline**: 3-4 hours
**Status**: ðŸ“‹ Planned

**Technical Specifications**:
```python
# Model Configuration
model_name = "efficientnetv2_b3.in21k_ft_in1k"
input_size = (256, 256)
num_classes = 1

# Training Configuration
batch_size = 28  # Adjusted for EfficientNetV2 memory requirements
learning_rate = 5e-5  # Lower LR for larger model
weight_decay = 1e-4
epochs = 50
optimizer = "AdamW"
scheduler = "CosineAnnealingLR"

# Output Directory
output_dir = "output/stage2_effnet/"
```

**Implementation Points**:
- Base structure copied from `train_stage1.py`
- Model replacement: MobileNetV4 â†’ EfficientNetV2-B3
- Memory optimization for larger model
- Same data augmentation strategy as Stage 1
- AUC-based model selection

**Expected Performance**:
- Target AUC: >0.90
- Target F1-Score: >0.85
- Training time: ~3-4 hours on GPU

### Task 2.2: GenConViT Model Training â³
**Script**: `src/stage2/train_stage2_genconvit.py`
**Timeline**: 4-5 hours (æ›´å¤æ‚çš„æž¶æž„)
**Status**: ðŸ“‹ Planned

**Technical Specifications**:
```python
# Model Configuration
model_variant = "GenConViTED"  # æˆ– GenConViTVAE
input_size = (256, 256)
num_classes = 1

# Training Configuration
batch_size = 16  # æ›´ä¿å®ˆ (autoencoder + transformer)
learning_rate = 1e-4  # è®ºæ–‡æŽ¨èå€¼
weight_decay = 1e-4
epochs = 50
optimizer = "Adam"  # GenConViTè®ºæ–‡ä½¿ç”¨
scheduler = "StepLR"
step_size = 10
gamma = 0.1

# æŸå¤±æƒé‡é…ç½®
autoencoder_loss_weight = 0.1
classification_loss_weight = 0.9
vae_kl_loss_weight = 0.01  # ä»…VAEå˜ä½“

# Output Directory
output_dir = "output/stage2_genconvit/"
```

**Implementation Points**:
- GenConViTåŒç½‘ç»œæž¶æž„ (ED/VAEå˜ä½“)
- ConvNeXt-Swinæ··åˆå±‚å®žçŽ°
- è‡ªç¼–ç å™¨é‡å»ºæŸå¤± + åˆ†ç±»æŸå¤±
- é¢„è®­ç»ƒæ¨¡åž‹é›†æˆ (Hugging Face: Deressa/GenConViT)

**Expected Performance**:
- Target AUC: >0.93 (åŸºäºŽè®ºæ–‡99.3% AUC)
- Target F1-Score: >0.88
- Training time: ~4-5 hours on GPU

### Task 2.3: Feature Extractor Implementation â³
**Script**: `src/stage2/feature_extractor.py`
**Timeline**: 2-3 hours (GenConViTå¤æ‚ç‰¹å¾æå–)
**Status**: ðŸ“‹ Planned

**Core Functionality**:
```python
class FeatureExtractor:
    def __init__(self, model_name, checkpoint_path):
        # Load model with num_classes=0 (no classification head)
        # Load trained weights
        
    def extract(self, dataloader):
        # Extract embeddings from pre-classification layers
        # Return (embeddings, labels) as numpy arrays

class GenConViTFeatureExtractor:
    def __init__(self, model_path, variant="GenConViTED"):
        # åŠ è½½GenConViTæ¨¡åž‹
        # ç§»é™¤æœ€ç»ˆåˆ†ç±»å¤´ï¼Œä¿ç•™ç‰¹å¾æå–éƒ¨åˆ†
        
    def extract_features(self, dataloader):
        # ä»ŽConvNeXt-Swinæ··åˆå±‚æå–ç‰¹å¾
        # ä»Žè‡ªç¼–ç å™¨ç»„ä»¶æå–é‡å»ºç‰¹å¾
        # è¿”å›žå¤šå±‚æ¬¡ç‰¹å¾ç»„åˆ
```

**Technical Requirements**:
- Support for both timm and GenConViT architectures
- Automatic classification head removal
- Multi-component feature extraction (ConvNeXt + Swin + AE features)
- Batch processing with GPU acceleration
- Memory-efficient embedding extraction

### Task 2.4: Utils and Evaluation Scripts â³
**Scripts**: `src/stage2/utils.py`, `src/stage2/evaluate_stage2.py`
**Timeline**: 1-2 hours
**Status**: ðŸ“‹ Planned

**Utils Functions**:
- Checkpoint loading utilities for both timm and GenConViT models
- Evaluation metrics calculation
- Feature dimension alignment tools
- GenConViT-specific visualization helpers

**Evaluation Features**:
- Individual model performance assessment
- Cross-model complementarity analysis
- Embedding quality validation
- GenConViT reconstruction quality metrics
- Stage 3 preparation metrics

## ðŸ“ Stage 2 Directory Structure

```
src/stage2/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ train_stage2_effnet.py        # EfficientNetV2-B3è®­ç»ƒ
â”œâ”€â”€ train_stage2_genconvit.py     # GenConViTè®­ç»ƒ
â”œâ”€â”€ genconvit/                    # GenConViTæºç é›†æˆ (æ–°)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py                  # GenConViTæ¨¡åž‹å®šä¹‰
â”‚   â”œâ”€â”€ config.py                 # é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ utils.py                  # GenConViTå·¥å…·å‡½æ•°
â”œâ”€â”€ feature_extractor.py          # å‡çº§æ”¯æŒGenConViT
â”œâ”€â”€ utils.py                      # é€šç”¨å·¥å…·å‡½æ•°
â””â”€â”€ evaluate_stage2.py            # é˜¶æ®µäºŒè¯„ä¼°

output/stage2/
â”œâ”€â”€ effnet/                       # EfficientNetV2-B3è¾“å‡º
â”‚   â”œâ”€â”€ best_model.pth
â”‚   â”œâ”€â”€ training_log.csv
â”‚   â”œâ”€â”€ training_curves.png
â”‚   â””â”€â”€ evaluation_results.json
â””â”€â”€ genconvit/                    # GenConViTè¾“å‡º (æ–°)
    â”œâ”€â”€ best_model_ed.pth         # EDå˜ä½“
    â”œâ”€â”€ best_model_vae.pth        # VAEå˜ä½“ (å¯é€‰)
    â”œâ”€â”€ training_log.csv
    â”œâ”€â”€ reconstruction_samples.png # é‡å»ºè´¨é‡æ ·æœ¬
    â””â”€â”€ evaluation_results.json
```

## ðŸ”§ Technical Implementation Strategy

### **Memory Management**
- **EfficientNetV2-B3**: Batch size 28 (balanced for ~12GB VRAM)
- **GenConViT**: Batch size 16 (conservative for autoencoder + attention)
- **Gradient Accumulation**: If needed for larger effective batch sizes

### **Learning Rate Strategy**
- **EfficientNetV2**: 5e-5 (proven for large CNN models)
- **GenConViT**: 1e-4 (è®ºæ–‡æŽ¨èï¼ŒAdamä¼˜åŒ–å™¨)
- **GenConViT Scheduler**: StepLR with step_size=10, gamma=0.1

### **Data Pipeline Reuse**
- Same data augmentation as Stage 1
- Same manifest files (train_manifest.csv, val_manifest.csv)
- Consistent 256Ã—256 input format
- Same normalization parameters

### **GenConViT Integration Strategy**
```python
# é¢„è®­ç»ƒæ¨¡åž‹åŠ è½½
from transformers import AutoModel
pretrained_model = AutoModel.from_pretrained("Deressa/GenConViT")

# æˆ–è€…ä»ŽGitHubæºç é›†æˆ
git clone https://github.com/erprogs/GenConViT
# é›†æˆåˆ° src/stage2/genconvit/
```

## ðŸŽ¯ Success Criteria

### **Performance Targets**
- **EfficientNetV2-B3 AUC**: >0.90
- **GenConViT AUC**: >0.93 (åŸºäºŽè®ºæ–‡æ€§èƒ½)
- **Individual Model F1**: >0.85 (EfficientNetV2), >0.88 (GenConViT)
- **Complementarity**: Different failure modes between CNN and generative methods
- **Feature Quality**: Rich embedding vectors for meta-learning

### **Integration Readiness**
- **Feature Extraction**: Clean embedding extraction from both architectures
- **Dimension Consistency**: Unified feature space (256-D after projection)
- **GenConViT Features**: Multi-modal features (reconstruction + attention + CNN)
- **Stage 3 Preparation**: Ready for K-Fold cross-validation pipeline

## ðŸ“ˆ Expected Timeline

| Task | Duration | Dependencies | Status |
|------|----------|--------------|--------|
| **Directory Setup** | 15 min | Stage 1 complete | ðŸ“‹ Planned |
| **GenConViT Integration** | 2-3 hours | Directory setup | ðŸ“‹ Planned |
| **Task 2.1: EfficientNetV2** | 3-4 hours | Directory setup | ðŸ“‹ Planned |
| **Task 2.2: GenConViT** | 4-5 hours | GenConViT integrated | ðŸ“‹ Planned |
| **Task 2.3: Feature Extractor** | 2-3 hours | Both models trained | ðŸ“‹ Planned |
| **Task 2.4: Utils & Evaluation** | 1-2 hours | Feature extractor ready | ðŸ“‹ Planned |
| **Integration Testing** | 1 hour | All tasks complete | ðŸ“‹ Planned |

**Total Estimated Time**: 13-18 hours (å¢žåŠ äº†GenConViTé›†æˆå¤æ‚åº¦)

## ðŸ”— Stage 3 Integration Preparation

### **Embedding Vector Strategy**
- **EfficientNetV2 Features**: CNN-based local texture features
- **GenConViT Features**: Multi-modal features
  - ConvNeXt-Swin attention features
  - Autoencoder reconstruction features  
  - VAE latent features (if using VAE variant)
- **Feature Fusion**: Concatenation after dimension alignment to 256-D
- **Meta-Learning Ready**: Prepared for LightGBM meta-model

### **Heterogeneous Architecture Comparison**
| æ–¹é¢ | EfficientNetV2-B3 | GenConViT |
|------|------------------|-----------|
| **æž¶æž„ç±»åž‹** | çº¯CNN | æ··åˆ(CNN+Transformer+ç”Ÿæˆå¼) |
| **ä¸“é•¿é¢†åŸŸ** | å±€éƒ¨çº¹ç†å¼‚å¸¸ | å…¨å±€è¯­ä¹‰+ç”Ÿæˆå¼ä¼ªå½± |
| **ç‰¹å¾ç±»åž‹** | å·ç§¯ç‰¹å¾ | å¤šæ¨¡æ€ç‰¹å¾(é‡å»º+æ³¨æ„åŠ›) |
| **è®¡ç®—å¤æ‚åº¦** | ä¸­ç­‰ | è¾ƒé«˜ |
| **æ£€æµ‹æœºåˆ¶** | åˆ¤åˆ«å¼ | ç”Ÿæˆå¼+åˆ¤åˆ«å¼ |

### **K-Fold Readiness**
- **Model Retraining**: Both models ready for 5-fold cross-validation
- **Feature Extraction**: Automated embedding generation per fold
- **GenConViT Complexity**: Handle multi-component architecture in K-fold
- **Data Leakage Prevention**: Strict train/validation separation

## ðŸ“ Next Action Items

1. **Create Stage 2 Directory Structure**
2. **Integrate GenConViT Source Code**
3. **Implement EfficientNetV2-B3 Training Script**
4. **Implement GenConViT Training Script**
5. **Develop Enhanced Feature Extraction Framework**
6. **Build Evaluation and Utils for both architectures**
7. **Conduct Integration Testing**

## ðŸŽ¯ Stage 2 Success Definition

Upon completion, Stage 2 will deliver:
- âœ… Two high-performance expert models (EfficientNetV2 AUC >0.90, GenConViT AUC >0.93)
- âœ… True heterogeneous complementarity (CNN vs Generative-Discriminative)
- âœ… Multi-modal feature extraction pipeline (CNN + Attention + Reconstruction)
- âœ… Ready for Stage 3 stacking ensemble with rich feature diversity
- âœ… Comprehensive evaluation and documentation

This foundation will enable Stage 3 to build a meta-model that intelligently combines the strengths of both local texture analysis (EfficientNetV2) and global generative-discriminative analysis (GenConViT), achieving superior performance compared to any individual model.