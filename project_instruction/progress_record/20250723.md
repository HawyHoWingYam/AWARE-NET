# Stage 1 Implementation Plan - Fast Filter (2025-07-23)

## üéØ Stage 1 Overall Objective
Build the first line of defense for the cascade system - a fast filter based on MobileNetV4-Hybrid-Medium to efficiently filter "simple samples" and lay the foundation for the subsequent precision analyzer.

## üìä Current Project Status
- ‚úÖ **Stage 0 Completed**: Environment setup, data preprocessing, multi-threading GPU acceleration
- ‚úÖ **Data Ready**: 256x256 PNG format, including CelebDF-v2, FF++, DFDC, DF40
- ‚úÖ **Cleanup Completed**: Project structure optimized, 23 redundant scripts cleaned
- ‚úÖ **Infrastructure**: PyTorch 2.7.1+cu128 environment, GPU utilization 70-85%

## üöÄ Stage 1 Three Core Tasks

### Task 1.1: Model Training (train_stage1.py)
**Objective**: Fine-tune MobileNetV4-Hybrid-Medium model to establish fast filter baseline

**Implementation Points**:
- Use timm library to load `mobilenetv4_hybrid_medium.ix_e550_r256_in1k` pre-trained model
- Modify classification head for binary classification (output 1 node)
- Data loading: Read `train_manifest.csv` and `val_manifest.csv`
- Data augmentation: RandomHorizontalFlip, ColorJitter, RandomAffine, GaussianBlur
- Training config: BCEWithLogitsLoss + AdamW + CosineAnnealingLR
- Model saving: Auto-save best weights based on validation AUC
- Metrics logging: Loss, Accuracy, AUC, F1-Score

**Key Technical Details**:
```python
# Model configuration
model_name = "mobilenetv4_hybrid_medium.ix_e550_r256_in1k"
input_size = (256, 256)  # Match processed data format
num_classes = 1  # Binary classification

# Data augmentation strategy
train_transforms = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),
    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Training configuration
loss_fn = torch.nn.BCEWithLogitsLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
```

**Expected Output**:
- `output/stage1/best_model.pth` - Best model weights
- `output/stage1/training_log.csv` - Training logs
- Validation AUC baseline metrics

### Task 1.2: Probability Calibration (calibrate_model.py)
**Objective**: Use temperature scaling to calibrate model probability output, improving confidence reliability

**Implementation Points**:
- Load best model weights from Task 1.1
- Obtain model logits on validation set
- Implement temperature scaling optimization: `calibrated_prob = sigmoid(logits / T)`
- Use scipy.optimize.minimize to find optimal temperature T
- Objective function: Minimize NLL or ECE
- Generate reliability diagrams before and after calibration

**Key Technical Details**:
```python
# Temperature scaling implementation
def temperature_scaling(logits, temperature):
    return torch.sigmoid(logits / temperature)

# Optimization objective function - Negative Log-Likelihood
def nll_loss(temperature, logits, labels):
    scaled_logits = logits / temperature
    return F.binary_cross_entropy_with_logits(scaled_logits, labels)

# Find optimal temperature
from scipy.optimize import minimize
result = minimize(nll_loss, x0=1.0, args=(logits, labels), 
                 bounds=[(0.1, 10.0)], method='L-BFGS-B')
optimal_temperature = result.x[0]
```

**Expected Output**:
- `output/stage1/calibration_temp.json` - Optimal temperature value T
- `output/stage1/reliability_diagram.png` - Calibration effect visualization
- Post-calibration probability distribution analysis

### Task 1.3: Performance Evaluation (evaluate_stage1.py)  
**Objective**: Comprehensive evaluation of calibrated model performance, providing data baseline for cascade strategy design

**Implementation Points**:
- Load calibrated model and temperature parameters
- Execute complete inference on validation set
- Calculate key metrics: AUC, F1-Score, Accuracy, Confusion Matrix, ECE
- Generate ROC curves and reliability plots
- Analyze sample distribution across different confidence intervals
- Provide data support for cascade threshold strategy (conservative threshold >0.98)

**Key Evaluation Metrics**:
```python
# Core metrics calculation
from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix

# Cascade strategy analysis
def analyze_cascade_thresholds(probs, labels):
    """Analyze filtration and leakage rates at different thresholds"""
    thresholds = [0.90, 0.95, 0.98, 0.99]
    results = {}
    
    for thresh in thresholds:
        # Simple real samples (high confidence real)
        simple_real = probs > thresh
        # Stage-1 leakage rate (fake samples incorrectly passed)
        fake_mask = labels == 1
        leakage_rate = (simple_real & fake_mask).sum() / fake_mask.sum()
        # Filtration rate
        filtration_rate = simple_real.sum() / len(probs)
        
        results[thresh] = {
            'leakage_rate': leakage_rate,
            'filtration_rate': filtration_rate
        }
    return results
```

**Expected Output**:
- `output/stage1/evaluation_report.json` - Complete evaluation report
- `output/stage1/roc_curve.png` - ROC curve plot
- Stage-1 Leakage Rate analysis

## üìÅ Directory Structure Plan
```
src/
‚îú‚îÄ‚îÄ stage1/              # New Stage 1 directory
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ train_stage1.py      # Task 1.1
‚îÇ   ‚îú‚îÄ‚îÄ calibrate_model.py   # Task 1.2
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_stage1.py   # Task 1.3
‚îÇ   ‚îî‚îÄ‚îÄ utils.py             # Shared utility functions
‚îú‚îÄ‚îÄ utils/               # Existing utilities
‚îÇ   ‚îî‚îÄ‚îÄ dataset_config.py
output/
‚îî‚îÄ‚îÄ stage1/              # Output directory
    ‚îú‚îÄ‚îÄ best_model.pth
    ‚îú‚îÄ‚îÄ calibration_temp.json
    ‚îî‚îÄ‚îÄ evaluation_report.json
```

## üîó Task Dependencies
1. **Task 1.1** ‚Üí **Task 1.2** ‚Üí **Task 1.3** (Sequential execution)
2. All tasks depend on Stage 0's processed_data/ directory
3. Use existing scripts/setup_dataset_config.py configuration system
4. Depend on existing manifest files: train_manifest.csv, val_manifest.csv

## üìà Success Metrics & Expected Results
- **Model Training Convergence**: Validation AUC > 0.85, F1-Score > 0.80
- **Significant Calibration Effect**: ECE reduction >50%, reliability diagram improvement
- **Cascade Strategy Data**: At 0.98 threshold, leakage rate <5%, filtration rate >30%
- **Baseline Establishment**: Provide clear performance baseline for Stage 2 heterogeneous ensemble

## ‚è±Ô∏è Implementation Timeline
- **Task 1.1**: Model Training (4-6 hours, including tuning and validation)
- **Task 1.2**: Probability Calibration (1-2 hours, including optimization and visualization)  
- **Task 1.3**: Performance Evaluation (1 hour, including report generation)
- **Total**: 6-9 hours to complete all Stage 1 tasks

## üéØ Next Action Plan
1. Create src/stage1/ directory structure and initial files
2. Implement train_stage1.py training script
3. Implement calibrate_model.py calibration script
4. Implement evaluate_stage1.py evaluation script
5. Run complete Stage 1 pipeline and generate performance report

## üìù Technical Notes
- **Conservative Strategy**: Use >0.98 high threshold to ensure Stage 1 doesn't miss fake samples
- **Probability Calibration**: Temperature scaling is simple and effective for binary classification
- **Evaluation Focus**: Focus on AUC and leakage rate, key metrics for cascade architecture
- **Data Format**: All inputs are 256x256 PNG format, consistent with DF40 standard

## üîç Key Technical Decisions

### Model Architecture Choice
- **MobileNetV4-Hybrid-Medium**: Optimal balance between speed and accuracy for mobile deployment
- **Input Size**: 256x256 to match Stage 0 preprocessing output
- **Classification Head**: Single output node with BCEWithLogitsLoss for numerical stability

### Training Strategy
- **Data Augmentation**: Comprehensive augmentation to improve robustness without overfitting
- **Optimizer**: AdamW for better weight decay handling
- **Scheduler**: CosineAnnealingLR for smooth learning rate decay
- **Early Stopping**: Based on validation AUC to prevent overfitting

### Calibration Approach
- **Temperature Scaling**: Simple, effective, and preserves model ranking
- **Optimization Target**: NLL minimization for well-calibrated probabilities
- **Validation**: ECE and reliability diagrams for calibration quality assessment

### Evaluation Framework
- **Multi-Metric**: AUC (primary), F1-Score, Accuracy, ECE
- **Cascade Analysis**: Threshold-specific leakage and filtration rates
- **Visualization**: ROC curves and reliability plots for comprehensive understanding

This detailed implementation plan ensures successful realization of Stage 1 fast filter, laying a solid foundation for the entire cascade detection system.