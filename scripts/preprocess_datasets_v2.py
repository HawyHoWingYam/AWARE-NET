#!/usr/bin/env python3
"""
æ•°æ®é¢„å¤„ç†è„šæœ¬ V2 - preprocess_datasets_v2.py
ä½¿ç”¨çµæ´»çš„é…ç½®ç³»ç»Ÿå¤„ç†å¤šç§æ•°æ®é›†ç»“æ„

åŠŸèƒ½:
1. åŸºäºJSONé…ç½®æ–‡ä»¶çš„çµæ´»è·¯å¾„ç®¡ç†
2. æ”¯æŒä»»æ„æ•°æ®é›†ç›®å½•ç»“æ„
3. æ™ºèƒ½çš„æ•°æ®é›†è§£æå’Œå¤„ç†
4. è‡ªåŠ¨ç”Ÿæˆæ•°æ®æ¸…å•æ–‡ä»¶
"""

import os
import cv2
import json
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
import logging
from typing import Dict, List, Tuple, Optional
import argparse
import warnings
import concurrent.futures
import threading
import torch
warnings.filterwarnings('ignore')

# GPUè¦–é »è™•ç†åº«çš„å¯é¸å°å…¥
VIDEO_BACKENDS = {
    'opencv': True,  # é»˜èªå¯ç”¨
    'torchvision': False,
    'decord': False,
    'dali': False
}

# äººè‡‰æª¢æ¸¬åº«çš„å¯é¸å°å…¥
FACE_DETECTION_BACKENDS = {
    'facenet_pytorch': False,
    'insightface': False,
    'mtcnn': False
}

try:
    import torchvision.io as tvio
    VIDEO_BACKENDS['torchvision'] = True
    print("âœ… TorchVision.ioå¯ç”¨ - GPUè¦–é »è™•ç†")
except ImportError:
    pass

try:
    import decord
    decord.bridge.set_bridge('torch')
    VIDEO_BACKENDS['decord'] = True  
    print("âœ… Decordå¯ç”¨ - é«˜æ•ˆGPUè¦–é »è™•ç†ï¼ˆWindowsæ¨è–¦ï¼‰")
except ImportError:
    pass

try:
    from nvidia.dali import pipeline_def, fn
    VIDEO_BACKENDS['dali'] = True
    print("âœ… NVIDIA DALIå¯ç”¨ - é«˜æ€§èƒ½GPUè¦–é »è™•ç†")
except ImportError:
    pass

# äººè‡‰æª¢æ¸¬åº«æª¢æ¸¬
try:
    from facenet_pytorch import MTCNN
    FACE_DETECTION_BACKENDS['facenet_pytorch'] = True
    print("âœ… facenet-pytorchå¯ç”¨")
except ImportError:
    print("âš ï¸ facenet-pytorchä¸å¯ç”¨ï¼ˆèˆ‡PyTorch 2.7+ä¸å…¼å®¹ï¼‰")

try:
    import insightface
    FACE_DETECTION_BACKENDS['insightface'] = True
    print("âœ… InsightFaceå¯ç”¨ - é«˜æ€§èƒ½äººè‡‰æª¢æ¸¬")
except ImportError:
    pass

try:
    import mtcnn
    FACE_DETECTION_BACKENDS['mtcnn'] = True
    print("âœ… MTCNNå¯ç”¨")
except ImportError:
    pass

# æª¢æŸ¥OpenCV DNNäººè‡‰æª¢æ¸¬
try:
    import cv2
    # æª¢æŸ¥æ˜¯å¦å¯ä»¥è¼‰å…¥DNNæ¨¡å‹
    FACE_DETECTION_BACKENDS['opencv_dnn'] = True
    print("âœ… OpenCV DNNäººè‡‰æª¢æ¸¬å¯ç”¨ï¼ˆè¼•é‡ç´šï¼‰")
except:
    pass

# æª¢æŸ¥YOLOv8äººè‡‰æª¢æ¸¬
try:
    from ultralytics import YOLO
    FACE_DETECTION_BACKENDS['yolov8'] = True
    print("âœ… YOLOv8äººè‡‰æª¢æ¸¬å¯ç”¨ï¼ˆGPUåŠ é€Ÿï¼‰")
except ImportError:
    pass

# æª¢æŸ¥MediaPipeäººè‡‰æª¢æ¸¬
try:
    import mediapipe as mp
    FACE_DETECTION_BACKENDS['mediapipe'] = True
    print("âœ… MediaPipeäººè‡‰æª¢æ¸¬å¯ç”¨ï¼ˆGPUåŠ é€Ÿï¼‰")
except ImportError:
    pass

# æª¢æŸ¥RetinaFaceäººè‡‰æª¢æ¸¬
try:
    from retinaface import RetinaFace
    FACE_DETECTION_BACKENDS['retinaface'] = True
    print("âœ… RetinaFaceäººè‡‰æª¢æ¸¬å¯ç”¨ï¼ˆå°ˆæ¥­ç´šGPUï¼‰")
except (ImportError, ValueError) as e:
    if "tf-keras" in str(e):
        print("âš ï¸ RetinaFaceéœ€è¦tf-keras: pip install tf-keras")
    else:
        print(f"âš ï¸ RetinaFaceä¸å¯ç”¨: {e}")

# å¯¼å…¥é…ç½®ç®¡ç†å™¨
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))
from utils.dataset_config import DatasetPathConfig

# =============================================================================
# é€šç”¨äººè‡‰æª¢æ¸¬å™¨é¡
# =============================================================================

class UniversalFaceDetector:
    """é€šç”¨äººè‡‰æª¢æ¸¬å™¨ï¼Œæ”¯æ´å¤šç¨®å¾Œç«¯"""
    
    def __init__(self, backend='auto', device='cuda', **kwargs):
        self.device = device
        self.backend = self._select_backend(backend)
        self.detector = self._initialize_detector(**kwargs)
        print(f"ğŸ” ä½¿ç”¨äººè‡‰æª¢æ¸¬å¾Œç«¯: {self.backend}")
    
    def _select_backend(self, preferred):
        """é¸æ“‡æœ€ä½³çš„äººè‡‰æª¢æ¸¬å¾Œç«¯"""
        if preferred != 'auto' and FACE_DETECTION_BACKENDS.get(preferred, False):
            return preferred
        
        # æŒ‰æ€§èƒ½å’Œå…¼å®¹æ€§å„ªå…ˆç´šé¸æ“‡ï¼ˆäººè‡‰æª¢æ¸¬å°ˆç”¨å„ªå…ˆï¼‰
        priority = ['insightface', 'mediapipe', 'facenet_pytorch', 'mtcnn', 'opencv_dnn', 'yolov8']
        for backend in priority:
            if FACE_DETECTION_BACKENDS.get(backend, False):
                return backend
        
        available_backends = [k for k, v in FACE_DETECTION_BACKENDS.items() if v]
        if not available_backends:
            error_msg = """
âŒ æ²’æœ‰å¯ç”¨çš„äººè‡‰æª¢æ¸¬å¾Œç«¯ï¼

æ¨è–¦å®‰è£æ–¹æ¡ˆï¼ˆæŒ‰å„ªå…ˆç´šï¼‰ï¼š
1. InsightFaceï¼ˆæ¨è–¦ï¼Œèˆ‡PyTorch 2.7+å®Œå…¨å…¼å®¹ï¼‰ï¼š
   pip install insightface onnxruntime-gpu

2. MTCNNï¼ˆå‚™ç”¨é¸æ“‡ï¼‰ï¼š
   pip install mtcnn

3. facenet-pytorchï¼ˆä¸æ¨è–¦ï¼Œèˆ‡PyTorch 2.7+æœ‰è¡çªï¼‰ï¼š
   pip install facenet-pytorch --force-reinstall --no-deps
            """
            raise RuntimeError(error_msg)
        
        raise RuntimeError(f"äººè‡‰æª¢æ¸¬å¾Œç«¯ '{preferred}' ä¸å¯ç”¨ã€‚å¯ç”¨å¾Œç«¯: {available_backends}")
    
    def _initialize_detector(self, **kwargs):
        """åˆå§‹åŒ–æª¢æ¸¬å™¨"""
        if self.backend == 'yolov8':
            return self._init_yolov8(**kwargs)
        elif self.backend == 'mediapipe':
            return self._init_mediapipe(**kwargs)
        elif self.backend == 'opencv_dnn':
            return self._init_opencv_dnn(**kwargs)
        elif self.backend == 'insightface':
            return self._init_insightface(**kwargs)
        elif self.backend == 'facenet_pytorch':
            return self._init_facenet_pytorch(**kwargs)
        elif self.backend == 'mtcnn':
            return self._init_mtcnn(**kwargs)
        else:
            raise RuntimeError(f"ä¸æ”¯æ´çš„äººè‡‰æª¢æ¸¬å¾Œç«¯: {self.backend}")
    
    def _init_insightface(self, **kwargs):
        """åˆå§‹åŒ–InsightFaceæª¢æ¸¬å™¨ï¼ˆåƒ…æª¢æ¸¬ï¼Œä¸åšç‰¹å¾µæå–ï¼‰"""
        import insightface
        
        # åªå•Ÿç”¨æª¢æ¸¬æ¨¡å‹ï¼Œç¦ç”¨ç‰¹å¾µæå–ä»¥æå‡é€Ÿåº¦
        app = insightface.app.FaceAnalysis(
            providers=['CUDAExecutionProvider', 'CPUExecutionProvider'] if self.device == 'cuda' else ['CPUExecutionProvider'],
            allowed_modules=['detection']  # åªå•Ÿç”¨æª¢æ¸¬æ¨¡å¡Š
        )
        app.prepare(ctx_id=0 if self.device == 'cuda' else -1, det_size=(640, 640))
        
        return app
    
    def _init_yolov8(self, **kwargs):
        """åˆå§‹åŒ–YOLOv8æª¢æ¸¬å™¨ï¼ˆé«˜æ€§èƒ½GPUï¼‰"""
        from ultralytics import YOLO
        
        try:
            # ä½¿ç”¨YOLOv8é€šç”¨æª¢æ¸¬æ¨¡å‹ï¼ˆå¯ä»¥æª¢æ¸¬äººè‡‰ï¼‰
            model = YOLO('yolov8n.pt')  # æœƒè‡ªå‹•ä¸‹è¼‰é€šç”¨æª¢æ¸¬æ¨¡å‹
            
            # è¨­ç½®è¨­å‚™
            if self.device == 'cuda':
                model.to('cuda')
                print("âœ… YOLOv8ä½¿ç”¨GPU")
            else:
                print("âœ… YOLOv8ä½¿ç”¨CPU")
            
            return {
                'model': model,
                'confidence_threshold': kwargs.get('confidence_threshold', 0.5)
            }
        except Exception as e:
            print(f"YOLOv8åˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    def _init_mediapipe(self, **kwargs):
        """åˆå§‹åŒ–MediaPipeäººè‡‰æª¢æ¸¬å™¨ï¼ˆGoogle GPUå„ªåŒ–ï¼‰"""
        import mediapipe as mp
        
        # åˆå§‹åŒ–MediaPipeäººè‡‰æª¢æ¸¬
        mp_face_detection = mp.solutions.face_detection
        mp_drawing = mp.solutions.drawing_utils
        
        face_detection = mp_face_detection.FaceDetection(
            model_selection=1,  # 0: 2må…§æª¢æ¸¬, 1: 5må…§æª¢æ¸¬
            min_detection_confidence=kwargs.get('confidence_threshold', 0.5)
        )
        
        print("âœ… MediaPipeäººè‡‰æª¢æ¸¬åˆå§‹åŒ–æˆåŠŸ")
        
        return {
            'detector': face_detection,
            'mp_face_detection': mp_face_detection,
            'confidence_threshold': kwargs.get('confidence_threshold', 0.5)
        }
    
    def _init_opencv_dnn(self, **kwargs):
        """åˆå§‹åŒ–OpenCV DNNäººè‡‰æª¢æ¸¬å™¨ï¼ˆè¼•é‡ç´šï¼Œå¿«é€Ÿï¼‰"""
        import cv2
        import urllib.request
        import os
        
        # æ¨¡å‹æ–‡ä»¶è·¯å¾‘
        model_dir = "weights"
        os.makedirs(model_dir, exist_ok=True)
        
        prototxt_path = os.path.join(model_dir, "deploy.prototxt")
        model_path = os.path.join(model_dir, "res10_300x300_ssd_iter_140000.caffemodel")
        
        # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œä¸‹è¼‰å®ƒå€‘
        if not os.path.exists(prototxt_path):
            print("ä¸‹è¼‰OpenCV DNNäººè‡‰æª¢æ¸¬æ¨¡å‹...")
            prototxt_url = "https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt"
            urllib.request.urlretrieve(prototxt_url, prototxt_path)
        
        if not os.path.exists(model_path):
            model_url = "https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel"
            urllib.request.urlretrieve(model_url, model_path)
        
        # è¼‰å…¥æ¨¡å‹
        net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)
        
        # ç°¡åŒ–è¨­ç½®ï¼šç›´æ¥ä½¿ç”¨CPUï¼ˆç©©å®šä¸”å¿«é€Ÿï¼‰
        net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)
        net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)
        print("âœ… OpenCV DNNä½¿ç”¨CPUï¼ˆç©©å®šæ¨¡å¼ï¼‰")
        
        return {
            'net': net,
            'confidence_threshold': kwargs.get('confidence_threshold', 0.5)
        }
    
    def _init_facenet_pytorch(self, **kwargs):
        """åˆå§‹åŒ–facenet-pytorchæª¢æ¸¬å™¨"""
        try:
            from facenet_pytorch import MTCNN
            
            return MTCNN(
                min_face_size=kwargs.get("min_face_size", 20),
                thresholds=kwargs.get("thresholds", [0.6, 0.7, 0.7]),
                factor=kwargs.get("factor", 0.709),
                post_process=kwargs.get("post_process", True),
                device=self.device,
                select_largest=False,
                keep_all=True
            )
        except Exception as e:
            print(f"âš ï¸ facenet-pytorchåˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    def _init_mtcnn(self, **kwargs):
        """åˆå§‹åŒ–MTCNNæª¢æ¸¬å™¨"""
        import mtcnn
        
        return mtcnn.MTCNN(
            min_face_size=kwargs.get("min_face_size", 20),
            thresholds=kwargs.get("thresholds", [0.6, 0.7, 0.7]),
            factor=kwargs.get("factor", 0.709),
            device=self.device
        )
    
    def detect(self, image):
        """çµ±ä¸€çš„äººè‡‰æª¢æ¸¬ä»‹é¢"""
        if self.backend == 'yolov8':
            return self._detect_yolov8(image)
        elif self.backend == 'mediapipe':
            return self._detect_mediapipe(image)
        elif self.backend == 'opencv_dnn':
            return self._detect_opencv_dnn(image)
        elif self.backend == 'insightface':
            return self._detect_insightface(image)
        elif self.backend == 'facenet_pytorch':
            return self._detect_facenet_pytorch(image)
        elif self.backend == 'mtcnn':
            return self._detect_mtcnn(image)
    
    def _detect_yolov8(self, image):
        """YOLOv8æª¢æ¸¬ï¼ˆé«˜æ€§èƒ½GPUï¼‰- æª¢æ¸¬äººè‡‰"""
        try:
            model = self.detector['model']
            confidence_threshold = self.detector['confidence_threshold']
            
            # YOLOv8æ¨ç†
            results = model(image, conf=confidence_threshold, verbose=False)
            
            boxes = []
            confidences = []
            
            # è§£æçµæœï¼Œéæ¿¾äººè‡‰é¡åˆ¥ (class_id = 0 for person)
            for result in results:
                if result.boxes is not None:
                    for box in result.boxes:
                        # æª¢æŸ¥é¡åˆ¥æ˜¯å¦ç‚ºäºº (class 0)
                        class_id = int(box.cls[0].cpu().numpy())
                        if class_id == 0:  # person class
                            # ç²å–é‚Šç•Œæ¡†åº§æ¨™ [x1, y1, x2, y2]
                            coords = box.xyxy[0].cpu().numpy()
                            confidence = box.conf[0].cpu().numpy()
                            
                            # å°‡äººé«”æª¢æ¸¬æ¡†ç•¶ä½œäººè‡‰å€åŸŸï¼ˆç²—ç•¥è¿‘ä¼¼ï¼‰
                            # å–ä¸ŠåŠéƒ¨åˆ†ä½œç‚ºäººè‡‰å€åŸŸ
                            x1, y1, x2, y2 = coords.astype(int)
                            face_height = int((y2 - y1) * 0.3)  # äººè‡‰ç´„å äººé«”ä¸Š30%
                            face_y2 = y1 + face_height
                            
                            boxes.append([x1, y1, x2, face_y2])
                            confidences.append(float(confidence))
            
            return boxes, confidences
            
        except Exception as e:
            print(f"YOLOv8æª¢æ¸¬éŒ¯èª¤: {e}")
            return None, None
    
    def _detect_mediapipe(self, image):
        """MediaPipeæª¢æ¸¬ï¼ˆGoogle GPUå„ªåŒ–ï¼‰"""
        try:
            detector = self.detector['detector']
            mp_face_detection = self.detector['mp_face_detection']
            
            # è½‰æ›BGRåˆ°RGB
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # æª¢æ¸¬äººè‡‰
            results = detector.process(rgb_image)
            
            boxes = []
            confidences = []
            
            if results.detections:
                h, w, _ = image.shape
                
                for detection in results.detections:
                    # ç²å–é‚Šç•Œæ¡†
                    bbox = detection.location_data.relative_bounding_box
                    
                    # è½‰æ›ç‚ºçµ•å°åº§æ¨™
                    x1 = int(bbox.xmin * w)
                    y1 = int(bbox.ymin * h)
                    x2 = int((bbox.xmin + bbox.width) * w)
                    y2 = int((bbox.ymin + bbox.height) * h)
                    
                    # ç²å–ç½®ä¿¡åº¦
                    confidence = detection.score[0]
                    
                    boxes.append([x1, y1, x2, y2])
                    confidences.append(float(confidence))
            
            return boxes, confidences
            
        except Exception as e:
            print(f"MediaPipeæª¢æ¸¬éŒ¯èª¤: {e}")
            return None, None
    
    def _detect_opencv_dnn(self, image):
        """OpenCV DNNæª¢æ¸¬ï¼ˆè¼•é‡ç´šï¼Œå¿«é€Ÿï¼‰"""
        try:
            net = self.detector['net']
            confidence_threshold = self.detector['confidence_threshold']
            
            (h, w) = image.shape[:2]
            
            # å‰µå»ºblob
            blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0,
                                       (300, 300), (104.0, 177.0, 123.0))
            
            # è¨­ç½®è¼¸å…¥ä¸¦åŸ·è¡Œå‰å‘å‚³æ’­
            net.setInput(blob)
            detections = net.forward()
            
            boxes = []
            confidences = []
            
            # è™•ç†æª¢æ¸¬çµæœ
            for i in range(0, detections.shape[2]):
                confidence = detections[0, 0, i, 2]
                
                if confidence > confidence_threshold:
                    # è¨ˆç®—é‚Šç•Œæ¡†åº§æ¨™
                    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
                    (x1, y1, x2, y2) = box.astype("int")
                    
                    boxes.append([x1, y1, x2, y2])
                    confidences.append(float(confidence))
            
            return boxes, confidences
            
        except Exception as e:
            print(f"OpenCV DNNæª¢æ¸¬éŒ¯èª¤: {e}")
            return None, None
    
    def _detect_insightface(self, image):
        """InsightFaceæª¢æ¸¬"""
        try:
            faces = self.detector.get(image)
            
            if not faces:
                return None, None
            
            # è½‰æ›ç‚ºèˆ‡MTCNNå…¼å®¹çš„æ ¼å¼
            boxes = []
            confidences = []
            
            for face in faces:
                bbox = face.bbox.astype(int)  # [x1, y1, x2, y2]
                confidence = face.det_score
                
                boxes.append(bbox)
                confidences.append(confidence)
            
            return boxes, confidences
            
        except Exception as e:
            print(f"InsightFaceæª¢æ¸¬éŒ¯èª¤: {e}")
            return None, None
    
    def _detect_facenet_pytorch(self, image):
        """facenet-pytorchæª¢æ¸¬"""
        try:
            boxes, probs = self.detector.detect(image)
            return boxes, probs
        except Exception as e:
            print(f"facenet-pytorchæª¢æ¸¬éŒ¯èª¤: {e}")
            return None, None
    
    def _detect_mtcnn(self, image):
        """MTCNNæª¢æ¸¬"""
        try:
            result = self.detector.detect_faces(image)
            
            if not result:
                return None, None
            
            boxes = []
            confidences = []
            
            for face in result:
                bbox = face['box']  # [x, y, w, h]
                confidence = face['confidence']
                
                # è½‰æ›ç‚º[x1, y1, x2, y2]æ ¼å¼
                x1, y1, w, h = bbox
                x2, y2 = x1 + w, y1 + h
                boxes.append([x1, y1, x2, y2])
                confidences.append(confidence)
            
            return boxes, confidences
            
        except Exception as e:
            print(f"MTCNNæª¢æ¸¬éŒ¯èª¤: {e}")
            return None, None

# =============================================================================
# GPUè¦–é »è™•ç†å™¨é¡
# =============================================================================

class GPUVideoProcessor:
    """GPUåŠ é€Ÿçš„è¦–é »è™•ç†å™¨"""
    
    def __init__(self, backend='auto', device='cuda'):
        self.device = device
        self.backend = self._select_backend(backend)
        logging.info(f"ğŸ¬ ä½¿ç”¨è¦–é »è™•ç†å¾Œç«¯: {self.backend}")
        
    def _select_backend(self, preferred):
        """é¸æ“‡æœ€ä½³çš„è¦–é »è™•ç†å¾Œç«¯"""
        if preferred != 'auto' and VIDEO_BACKENDS.get(preferred, False):
            return preferred
        
        # æŒ‰Windowsæ€§èƒ½å„ªå…ˆç´šé¸æ“‡ï¼ˆDALIåœ¨Windowsä¸Šæ”¯æ´æœ‰é™ï¼‰
        priority = ['decord', 'torchvision', 'dali', 'opencv']
        for backend in priority:
            if VIDEO_BACKENDS.get(backend, False):
                return backend
        return 'opencv'  # é»˜èªå›é€€
    
    def read_video_frames(self, video_path: str, frame_interval: int = 10, max_frames: int = None):
        """GPUåŠ é€Ÿçš„è¦–é »å¹€è®€å–"""
        if self.backend == 'torchvision':
            return self._read_with_torchvision(video_path, frame_interval, max_frames)
        elif self.backend == 'decord':
            return self._read_with_decord(video_path, frame_interval, max_frames)
        elif self.backend == 'dali':
            return self._read_with_dali(video_path, frame_interval, max_frames)
        else:
            return self._read_with_opencv(video_path, frame_interval, max_frames)
    
    def _read_with_torchvision(self, video_path, frame_interval, max_frames):
        """ä½¿ç”¨TorchVision.ioé€²è¡ŒGPUè¦–é »è™•ç†"""
        try:
            # è®€å–è¦–é »å…ƒæ•¸æ“š
            video_info = tvio.read_video_timestamps(video_path)
            total_frames = len(video_info[0])
            
            # è¨ˆç®—éœ€è¦çš„å¹€ç´¢å¼•
            frame_indices = list(range(0, total_frames, frame_interval))
            if max_frames:
                frame_indices = frame_indices[:max_frames]
            
            frames = []
            for frame_idx in frame_indices:
                # è®€å–å–®å¹€ (GPUç›´æ¥è™•ç†)
                frame_tensor, _, _ = tvio.read_video(
                    video_path, 
                    start_pts=frame_idx, 
                    end_pts=frame_idx + 1,
                    pts_unit='frame'
                )
                
                if frame_tensor.size(0) > 0:
                    # è½‰æ›ç‚ºnumpyæ ¼å¼ (MTCNNéœ€è¦)
                    frame = frame_tensor[0].permute(1, 2, 0).cpu().numpy()
                    frames.append((frame_idx, frame))
            
            logging.info(f"âœ… TorchVisionè®€å– {len(frames)} å¹€ (GPUåŠ é€Ÿ)")
            return frames
            
        except Exception as e:
            logging.warning(f"TorchVisionè®€å–å¤±æ•—: {e}ï¼Œå›é€€åˆ°OpenCV")
            return self._read_with_opencv(video_path, frame_interval, max_frames)
    
    def _read_with_decord(self, video_path, frame_interval, max_frames):
        """ä½¿ç”¨Decordé€²è¡Œè¦–é »è™•ç†ï¼ˆæ™ºèƒ½GPU/CPUé¸æ“‡ï¼‰"""
        try:
            # å…ˆå˜—è©¦GPUï¼Œå¦‚æœå¤±æ•—å‰‡ä½¿ç”¨CPU
            try:
                vr = decord.VideoReader(video_path, ctx=decord.gpu(0))
                gpu_used = True
            except:
                vr = decord.VideoReader(video_path, ctx=decord.cpu(0))
                gpu_used = False
                
            total_frames = len(vr)
            
            # è¨ˆç®—éœ€è¦çš„å¹€ç´¢å¼•
            frame_indices = list(range(0, total_frames, frame_interval))
            if max_frames:
                frame_indices = frame_indices[:max_frames]
            
            # æ‰¹é‡è®€å–å¹€
            frames_tensor = vr.get_batch(frame_indices)
            frames = []
            
            for i, frame_idx in enumerate(frame_indices):
                # è½‰æ›ç‚ºnumpy - è™•ç†ä¸åŒçš„tensoræ ¼å¼
                try:
                    if hasattr(frames_tensor, '__getitem__'):
                        # æ¨™æº–tensorç´¢å¼•
                        if gpu_used:
                            frame = frames_tensor[i].cpu().numpy()
                        else:
                            frame = frames_tensor[i].numpy()
                    else:
                        # å¦‚æœä¸æ”¯æŒç´¢å¼•ï¼Œå˜—è©¦å…¶ä»–æ–¹æ³•
                        frame = frames_tensor.asnumpy()[i] if hasattr(frames_tensor, 'asnumpy') else frames_tensor[i]
                    
                    # Decordè¿”å›RGBæ ¼å¼ï¼Œéœ€è¦è½‰æ›ç‚ºBGRä»¥ä¿æŒä¸€è‡´æ€§
                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                    frames.append((frame_idx, frame_bgr))
                    continue  # è·³éç•°å¸¸è™•ç†éƒ¨åˆ†
                    
                except Exception as e:
                    logging.warning(f"Frame {i} conversion failed: {e}, trying alternative method")
                    # å‚™ç”¨æ–¹æ³•ï¼šè½‰æ›æ•´å€‹batchç„¶å¾Œç´¢å¼•
                    if gpu_used and hasattr(frames_tensor, 'cpu'):
                        numpy_frames = frames_tensor.cpu().numpy()
                    else:
                        numpy_frames = frames_tensor.numpy() if hasattr(frames_tensor, 'numpy') else frames_tensor.asnumpy()
                    frame = numpy_frames[i]
                
                # Decordè¿”å›RGBæ ¼å¼ï¼Œéœ€è¦è½‰æ›ç‚ºBGRä»¥ä¿æŒä¸€è‡´æ€§
                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                frames.append((frame_idx, frame_bgr))
            
            gpu_status = "GPU" if gpu_used else "CPU"
            logging.info(f"âœ… Decordè®€å– {len(frames)} å¹€ ({gpu_status})")
            return frames
            
        except Exception as e:
            logging.warning(f"Decordè®€å–å¤±æ•—: {e}ï¼Œå›é€€åˆ°OpenCV")
            return self._read_with_opencv(video_path, frame_interval, max_frames)
    
    def _read_with_dali(self, video_path, frame_interval, max_frames):
        """ä½¿ç”¨NVIDIA DALIé€²è¡Œé«˜æ€§èƒ½GPUè¦–é »è™•ç†"""
        try:
            # DALI pipelineå®šç¾©
            @pipeline_def(batch_size=1, num_threads=2, device_id=0)
            def video_pipeline():
                video = fn.readers.video(
                    device="gpu",
                    file_root="",
                    filenames=[video_path],
                    sequence_length=max_frames or 100,
                    step=frame_interval,
                    normalized=False
                )
                return video
            
            pipe = video_pipeline()
            pipe.build()
            
            pipe_out = pipe.run()
            video_batch = pipe_out[0].as_cpu()  # ç§»åˆ°CPUé€²è¡Œäººè‡‰æª¢æ¸¬
            
            frames = []
            for i in range(video_batch.shape[0]):
                frame = np.array(video_batch[i])
                frames.append((i * frame_interval, frame))
            
            logging.info(f"âœ… DALIè®€å– {len(frames)} å¹€ (é«˜æ€§èƒ½GPU)")
            return frames
            
        except Exception as e:
            logging.warning(f"DALIè®€å–å¤±æ•—: {e}ï¼Œå›é€€åˆ°OpenCV")
            return self._read_with_opencv(video_path, frame_interval, max_frames)
    
    def _read_with_opencv(self, video_path, frame_interval, max_frames):
        """OpenCVå›é€€æ–¹æ¡ˆ (CPU)"""
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                return []
            
            frames = []
            frame_idx = 0
            count = 0
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                if frame_idx % frame_interval == 0:
                    frames.append((frame_idx, frame))
                    count += 1
                    
                    if max_frames and count >= max_frames:
                        break
                
                frame_idx += 1
            
            cap.release()
            logging.info(f"âš ï¸ OpenCVè®€å– {len(frames)} å¹€ (CPU)")
            return frames
            
        except Exception as e:
            logging.error(f"OpenCVè®€å–å¤±æ•—: {e}")
            return []

# =============================================================================
# æ ¸å¿ƒé¢„å¤„ç†ç±» V2
# =============================================================================

class DatasetPreprocessorV2:
    """æ•°æ®é›†é¢„å¤„ç†å™¨ V2 - ä½¿ç”¨é…ç½®ç³»ç»Ÿ"""
    
    def __init__(self, config_file: Optional[str] = None, video_backend: str = 'auto', face_detector: str = 'auto'):
        """
        åˆå§‹åŒ–é¢„å¤„ç†å™¨
        
        Args:
            config_file: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤çš„dataset_paths.json
            video_backend: è¦–é »è™•ç†å¾Œç«¯åå¥½
            face_detector: äººè‡‰æª¢æ¸¬å™¨å¾Œç«¯åå¥½
        """
        self.path_config = DatasetPathConfig(config_file)
        self.processing_config = self.path_config.get_processing_config()
        
        # è¨­ç½®å¾Œç«¯åå¥½
        self.processing_config['video_backend'] = video_backend
        if 'face_detector' not in self.processing_config:
            self.processing_config['face_detector'] = {}
        self.processing_config['face_detector']['backend'] = face_detector
        
        # ç·šç¨‹æœ¬åœ°å­˜å„²ï¼Œç”¨æ–¼å­˜å„²æ¯å€‹ç·šç¨‹çš„äººè‡‰æª¢æ¸¬å™¨å¯¦ä¾‹
        self._thread_local = threading.local()
        
        self.setup_directories()
        self.setup_video_processor()
        self.setup_face_detector()
        self.setup_logging()
    
    def get_thread_local_face_detector(self):
        """ç²å–ç·šç¨‹æœ¬åœ°çš„äººè‡‰æª¢æ¸¬å™¨å¯¦ä¾‹"""
        if not hasattr(self._thread_local, 'face_detector'):
            # ç‚ºæ¯å€‹ç·šç¨‹å‰µå»ºç¨ç«‹çš„äººè‡‰æª¢æ¸¬å™¨å¯¦ä¾‹
            detector_config = self.processing_config.get("face_detector", {})
            preferred_detector = detector_config.get("backend", "auto")
            
            # å¦‚æœåœ¨è™•ç†é…ç½®ä¸­æœ‰æŒ‡å®šï¼Œä½¿ç”¨å®ƒ
            if hasattr(self, 'processing_config') and 'face_detector' in self.processing_config:
                face_detector_config = self.processing_config.get('face_detector', {})
                if isinstance(face_detector_config, dict) and 'backend' in face_detector_config:
                    preferred_detector = face_detector_config['backend']
            
            device = 'cuda'
            
            logging.info(f"ğŸ”§ ç·šç¨‹ {threading.current_thread().name} å‰µå»ºäººè‡‰æª¢æ¸¬å™¨: {preferred_detector}")
            self._thread_local.face_detector = UniversalFaceDetector(
                backend=preferred_detector,
                device=device,
                min_face_size=detector_config.get("min_face_size", 20),
                thresholds=detector_config.get("thresholds", [0.6, 0.7, 0.7]),
                factor=detector_config.get("factor", 0.709),
                post_process=detector_config.get("post_process", True)
            )
            
        return self._thread_local.face_detector
    
    def set_workers(self, workers: int):
        """è¨­ç½®ä¸¦è¡Œå·¥ä½œç·šç¨‹æ•¸"""
        self._workers = workers
        
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        processed_path = self.path_config.config["base_paths"]["processed_data"]
        
        dirs_to_create = [
            f"{processed_path}/train/real",
            f"{processed_path}/train/fake", 
            f"{processed_path}/val/real",
            f"{processed_path}/val/fake",
            f"{processed_path}/final_test_sets/celebdf_v2/real",
            f"{processed_path}/final_test_sets/celebdf_v2/fake",
            f"{processed_path}/final_test_sets/ffpp/real",
            f"{processed_path}/final_test_sets/ffpp/fake",
            f"{processed_path}/final_test_sets/dfdc",
            f"{processed_path}/manifests"
        ]
        
        for dir_path in dirs_to_create:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
            
        logging.info(f"Created directory structure at {processed_path}")
    
    def setup_video_processor(self):
        """åˆå§‹åŒ–GPUè¦–é »è™•ç†å™¨"""
        try:
            preferred_backend = self.processing_config.get("video_backend", "auto")
            self.video_processor = GPUVideoProcessor(backend=preferred_backend, device='cuda')
            
            # æ‰“å°å¯ç”¨çš„è¦–é »è™•ç†å¾Œç«¯
            available_backends = [k for k, v in VIDEO_BACKENDS.items() if v]
            logging.info(f"ğŸ¬ å¯ç”¨è¦–é »å¾Œç«¯: {', '.join(available_backends)}")
            
        except Exception as e:
            logging.error(f"è¦–é »è™•ç†å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            # å‰µå»ºé»˜èªçš„OpenCVè™•ç†å™¨
            self.video_processor = GPUVideoProcessor(backend='opencv', device='cuda')
    
    def setup_face_detector(self):
        """åˆå§‹åŒ–é€šç”¨äººè„¸æ£€æµ‹å™¨ - æ”¯æŒå¤šç¨®å¾Œç«¯"""
        try:
            # æ£€æŸ¥CUDAå¯ç”¨æ€§
            import torch
            if not torch.cuda.is_available():
                raise RuntimeError("CUDAä¸å¯ç”¨ï¼æ­¤é¢„å¤„ç†è„šæœ¬éœ€è¦GPUæ”¯æŒã€‚è¯·å®‰è£…æ”¯æŒCUDAçš„PyTorchã€‚")
            
            # æ£€æŸ¥OpenCV CUDAæ”¯æŒ (å®‰å…¨æ£€æŸ¥)
            try:
                cuda_devices = cv2.cuda.getCudaEnabledDeviceCount()
                self.opencv_cuda_available = cuda_devices > 0
                
                if self.opencv_cuda_available:
                    logging.info(f"âœ… OpenCV CUDAæ”¯æŒå¯ç”¨: {cuda_devices} ä¸ªè®¾å¤‡")
                    # è®¾ç½®OpenCVä½¿ç”¨GPUå†…å­˜æ± 
                    cv2.cuda.setGlDevice(0)
                else:
                    logging.warning("âš ï¸ OpenCVæ²¡æœ‰CUDAæ”¯æŒ - è§†é¢‘è¯»å–å’Œå›¾åƒå¤„ç†å°†ä½¿ç”¨CPU")
                    logging.warning("æ³¨æ„ï¼šä¸»è¦çš„GPUåŠ é€Ÿ(äººè„¸æ£€æµ‹)ä»ç„¶å¯ç”¨")
            except AttributeError:
                # OpenCVæ²¡æœ‰ç¼–è¯‘CUDAæ”¯æŒ
                self.opencv_cuda_available = False
                logging.warning("âš ï¸ OpenCVç‰ˆæœ¬æ²¡æœ‰CUDAæ¨¡å— - ä½¿ç”¨CPUç‰ˆæœ¬")
                logging.info("âœ… äººè„¸æ£€æµ‹ä»å°†ä½¿ç”¨GPUåŠ é€Ÿ")
            
            detector_config = self.processing_config.get("face_detector", {})
            preferred_detector = detector_config.get("backend", "auto")
            
            # å¦‚æœåœ¨è™•ç†é…ç½®ä¸­æœ‰æŒ‡å®šï¼Œä½¿ç”¨å®ƒ
            if hasattr(self, 'processing_config') and 'face_detector' in self.processing_config:
                face_detector_config = self.processing_config.get('face_detector', {})
                if isinstance(face_detector_config, dict) and 'backend' in face_detector_config:
                    preferred_detector = face_detector_config['backend']
            
            # å¼ºåˆ¶ä½¿ç”¨CUDAè®¾å¤‡
            device = 'cuda'
            gpu_id = torch.cuda.current_device()
            gpu_name = torch.cuda.get_device_name(gpu_id)
            
            # å‰µå»ºé€šç”¨äººè‡‰æª¢æ¸¬å™¨
            self.face_detector = UniversalFaceDetector(
                backend=preferred_detector,
                device=device,
                min_face_size=detector_config.get("min_face_size", 20),
                thresholds=detector_config.get("thresholds", [0.6, 0.7, 0.7]),
                factor=detector_config.get("factor", 0.709),
                post_process=detector_config.get("post_process", True)
            )
            
            logging.info(f"âœ… äººè„¸æ£€æµ‹å™¨å·²åˆå§‹åŒ– - ä½¿ç”¨GPU: {gpu_name} (è®¾å¤‡ID: {gpu_id})")
            
            # æ‰“å°GPUå†…å­˜ä¿¡æ¯
            total_memory = torch.cuda.get_device_properties(gpu_id).total_memory / (1024**3)
            allocated_memory = torch.cuda.memory_allocated(gpu_id) / (1024**3)
            logging.info(f"ğŸ“Š GPUå†…å­˜: {allocated_memory:.2f}GB / {total_memory:.2f}GB")
            
            # é ç†±GPU
            self._warmup_gpu()
            
        except Exception as e:
            logging.error(f"âŒ äººè‡‰æª¢æ¸¬å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            logging.error("å¯ç”¨è§£æ±ºæ–¹æ¡ˆ:")
            logging.error("1. å®‰è£InsightFace: pip install insightface onnxruntime-gpu")
            logging.error("2. æˆ–å¼·åˆ¶å®‰è£facenet-pytorch: pip install facenet-pytorch --force-reinstall")
            logging.error("3. æˆ–å®‰è£MTCNN: pip install mtcnn")
            raise
    
    def _warmup_gpu(self):
        """é¢„çƒ­GPUä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½"""
        try:
            import torch
            logging.info("ğŸ”¥ é¢„çƒ­GPU...")
            
            # åˆ›å»ºä¸€ä¸ªdummyå›¾åƒè¿›è¡Œé¢„çƒ­
            dummy_image = torch.randint(0, 255, (480, 640, 3), dtype=torch.uint8)
            dummy_image_np = dummy_image.numpy()
            
            # é¢„çƒ­äººè„¸æ£€æµ‹å™¨
            self.face_detector.detect(dummy_image_np)
            
            logging.info("âœ… GPUé¢„çƒ­å®Œæˆ")
            
        except Exception as e:
            logging.warning(f"âš ï¸ GPUé¢„çƒ­å¤±è´¥: {e}")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        processed_path = self.path_config.config["base_paths"]["processed_data"]
        log_file = f"{processed_path}/preprocessing_v2.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
    
    def process_video(self, video_info: Dict) -> int:
        """
        å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œæå–äººè„¸å›¾åƒ
        
        Args:
            video_info: åŒ…å«è§†é¢‘è·¯å¾„ã€æ ‡ç­¾ã€åˆ’åˆ†ç­‰ä¿¡æ¯çš„å­—å…¸
            
        Returns:
            æå–çš„äººè„¸æ•°é‡
        """
        video_path = video_info["video_path"]
        label = video_info["label"]
        split = video_info["split"]
        dataset_name = video_info["dataset"]
        video_id = video_info["video_id"]
        
        if not os.path.exists(video_path):
            logging.warning(f"Video file not found: {video_path}")
            return 0
        
        # ç¡®å®šè¾“å‡ºç›®å½•
        processed_path = self.path_config.config["base_paths"]["processed_data"]
        
        if split == 'test' and dataset_name in ['celebdf_v2', 'ffpp']:
            output_dir = f"{processed_path}/final_test_sets/{dataset_name}/{label}"
        elif dataset_name == 'dfdc':
            output_dir = f"{processed_path}/final_test_sets/dfdc"
        else:
            output_dir = f"{processed_path}/{split}/{label}"
        
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        face_count = 0
        frame_interval = self.processing_config.get("frame_interval", 10)
        max_faces = self.processing_config.get("max_faces_per_video", 50)
        
        try:
            # ä½¿ç”¨GPUè¦–é »è™•ç†å™¨è®€å–å¹€
            max_video_frames = max_faces * 2  # ä¼°ç®—éœ€è¦çš„è¦–é »å¹€æ•¸
            frames = self.video_processor.read_video_frames(
                video_path, 
                frame_interval=frame_interval,
                max_frames=max_video_frames
            )
            
            if not frames:
                logging.error(f"ç„¡æ³•å¾è¦–é »è®€å–å¹€: {video_path}")
                return 0
            
            logging.info(f"ğŸ“¹ æˆåŠŸè®€å– {len(frames)} å¹€ï¼Œé–‹å§‹äººè‡‰æª¢æ¸¬")
            
            # è™•ç†æ¯ä¸€å¹€
            for frame_idx, frame in frames:
                if face_count >= max_faces:
                    break
                
                faces_extracted = self._extract_faces_from_frame(
                    frame, output_dir, video_id, frame_idx, face_count
                )
                face_count += faces_extracted
                
                # å®šæœŸæ¸…ç†GPUè¨˜æ†¶é«”
                if frame_idx % 100 == 0:
                    torch.cuda.empty_cache()
                
        except Exception as e:
            logging.error(f"Error processing video {video_path}: {e}")
            return 0
        
        return face_count
    
    def _extract_faces_from_frame(self, frame: np.ndarray, output_dir: str, 
                                 video_id: str, frame_idx: int, face_count: int) -> int:
        """ä»å•å¸§ä¸­æå–äººè„¸ - GPUåŠ é€Ÿç‰ˆæœ¬"""
        try:
            # ä½¿ç”¨GPUåŠ é€Ÿçš„å›¾åƒé¢„å¤„ç† (å¦‚æœOpenCVæ”¯æŒCUDA)
            if self.opencv_cuda_available:
                # ä¸Šä¼ åˆ°GPU
                gpu_frame = cv2.cuda_GpuMat()
                gpu_frame.upload(frame)
                
                # GPUä¸Šè¿›è¡Œé¢œè‰²è½¬æ¢
                gpu_rgb = cv2.cuda.cvtColor(gpu_frame, cv2.COLOR_BGR2RGB)
                
                # ä¸‹è½½åˆ°CPUè¿›è¡Œäººè„¸æ£€æµ‹ (MTCNNéœ€è¦numpy array)
                rgb_frame = gpu_rgb.download()
            else:
                # CPUé¢œè‰²è½¬æ¢
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # GPUäººè„¸æ£€æµ‹ - ä½¿ç”¨ç·šç¨‹æœ¬åœ°æª¢æ¸¬å™¨
            thread_face_detector = self.get_thread_local_face_detector()
            boxes, _ = thread_face_detector.detect(rgb_frame)
            
            if boxes is None:
                return 0
            
            extracted_count = 0
            max_faces = self.processing_config.get("max_faces_per_video", 50)
            image_size = tuple(self.processing_config.get("image_size", [256, 256]))  # DF40 format
            bbox_scale = self.processing_config.get("bbox_scale", 1.3)
            min_face_size = self.processing_config.get("min_face_size", 80)
            
            # æ‰¹é‡å¤„ç†äººè„¸è£å‰ªå’Œç¼©æ”¾
            faces_to_save = []
            
            for i, box in enumerate(boxes):
                if face_count + extracted_count >= max_faces:
                    break
                
                # è™•ç†ä¸åŒæª¢æ¸¬å™¨çš„é‚Šç•Œæ¡†æ ¼å¼
                if isinstance(box, (list, tuple)):
                    x1, y1, x2, y2 = [int(coord) for coord in box]
                else:
                    x1, y1, x2, y2 = box.astype(int)
                
                # è®¡ç®—æ‰©å±•åçš„è¾¹ç•Œæ¡†
                width = x2 - x1
                height = y2 - y1
                center_x = x1 + width // 2
                center_y = y1 + height // 2
                
                new_width = int(width * bbox_scale)
                new_height = int(height * bbox_scale)
                
                x1_new = max(0, center_x - new_width // 2)
                y1_new = max(0, center_y - new_height // 2)
                x2_new = min(frame.shape[1], center_x + new_width // 2)
                y2_new = min(frame.shape[0], center_y + new_height // 2)
                
                # æ£€æŸ¥äººè„¸å°ºå¯¸
                if (x2_new - x1_new) < min_face_size or (y2_new - y1_new) < min_face_size:
                    continue
                
                # GPUåŠ é€Ÿçš„äººè„¸è£å‰ªå’Œç¼©æ”¾ - ä¿®å¾©é¡è‰²é€šé“å•é¡Œ
                if self.opencv_cuda_available:
                    # åœ¨GPUä¸Šè¿›è¡Œè£å‰ªå’Œç¼©æ”¾ (ä½¿ç”¨BGRæ ¼å¼çš„åŸå§‹å¹€)
                    gpu_face_crop = gpu_frame[y1_new:y2_new, x1_new:x2_new]
                    gpu_face_resized = cv2.cuda.resize(gpu_face_crop, image_size)
                    face_resized = gpu_face_resized.download()  # BGRæ ¼å¼
                else:
                    # CPUå¤„ç† (ä½¿ç”¨BGRæ ¼å¼çš„åŸå§‹å¹€)
                    face_crop = frame[y1_new:y2_new, x1_new:x2_new]  # BGRæ ¼å¼
                    face_resized = cv2.resize(face_crop, image_size)  # BGRæ ¼å¼
                
                # å‡†å¤‡æ–‡ä»¶åå’Œè·¯å¾„
                filename = f"{video_id}_frame_{frame_idx:06d}_face_{i:02d}.png"
                output_path = os.path.join(output_dir, filename)
                
                faces_to_save.append((face_resized, output_path))
                extracted_count += 1
            
            # æ‰¹é‡ä¿å­˜ (å‡å°‘I/Oå¼€é”€) - ä½¿ç”¨ç•°æ­¥I/O
            saved_count = 0
            for face_data, output_path in faces_to_save:
                try:
                    success = cv2.imwrite(output_path, face_data, [cv2.IMWRITE_PNG_COMPRESSION, 1])
                    if success:
                        saved_count += 1
                    else:
                        logging.warning(f"Failed to save image: {output_path}")
                except Exception as e:
                    logging.warning(f"Error saving image {output_path}: {e}")
            
            # è¿”å›å¯¦éš›ä¿å­˜çš„æ•¸é‡
            extracted_count = saved_count
            
            return extracted_count
            
        except Exception as e:
            logging.error(f"Error extracting faces from frame: {e}")
            return 0
    
    def process_dataset(self, dataset_name: str) -> Dict[str, int]:
        """å¤„ç†æŒ‡å®šæ•°æ®é›† - å¤šç·šç¨‹ä¸¦è¡Œç‰ˆæœ¬"""
        logging.info(f"Starting processing of {dataset_name} dataset...")
        
        # è·³è¿‡DF40ï¼Œå› ä¸ºå®ƒå·²ç»æ˜¯é¢„å¤„ç†çš„å›¾åƒ
        if dataset_name == "df40":
            logging.info(f"Skipping {dataset_name} - already preprocessed images")
            return {"processed": 0, "total_faces": 0, "errors": 0, "skipped": True}
        
        try:
            video_paths = self.path_config.get_all_video_paths(dataset_name)
            logging.info(f"Found {len(video_paths)} videos in {dataset_name}")
            
            # è¨ˆç®—æœ€å„ªç·šç¨‹æ•¸ï¼ˆåŸºæ–¼CPUæ ¸å¿ƒæ•¸ï¼Œä½†é™åˆ¶ä»¥é¿å…éåº¦ç«¶çˆ­GPUï¼‰
            import multiprocessing
            if hasattr(self, '_workers') and self._workers > 0:
                max_workers = min(self._workers, 4)  # ç”¨æˆ¶æŒ‡å®šï¼Œä½†ä»é™åˆ¶æœ€å¤§å€¼
            else:
                max_workers = min(multiprocessing.cpu_count(), 4)  # é™åˆ¶ç‚º4å€‹ç·šç¨‹ä»¥å¹³è¡¡GPUä½¿ç”¨
            logging.info(f"ğŸš€ ä½¿ç”¨å¤šç·šç¨‹ä¸¦è¡Œè™•ç†: {max_workers} workers")
            
            stats = {"processed": 0, "total_faces": 0, "errors": 0}
            stats_lock = threading.Lock()
            
            def process_video_worker(video_info):
                """ç·šç¨‹å·¥ä½œå‡½æ•¸"""
                try:
                    face_count = self.process_video(video_info)
                    with stats_lock:
                        stats["total_faces"] += face_count
                        stats["processed"] += 1
                        
                        if stats["processed"] % 50 == 0:
                            # ç›£æ§GPUå…§å­˜ä½¿ç”¨
                            try:
                                import torch
                                if torch.cuda.is_available():
                                    allocated_memory = torch.cuda.memory_allocated() / (1024**3)
                                    cached_memory = torch.cuda.memory_reserved() / (1024**3)
                                    logging.info(f"Processed {stats['processed']} videos, extracted {stats['total_faces']} faces")
                                    logging.info(f"GPU Memory: {allocated_memory:.2f}GB allocated, {cached_memory:.2f}GB cached")
                            except Exception:
                                pass  # å¿½ç•¥å…§å­˜ç›£æ§éŒ¯èª¤
                    
                    return face_count
                except Exception as e:
                    with stats_lock:
                        stats["errors"] += 1
                    logging.error(f"Error processing video {video_info.get('video_path', 'unknown')}: {e}")
                    return 0
            
            # ä½¿ç”¨ThreadPoolExecutoré€²è¡Œä¸¦è¡Œè™•ç†
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»å‹™
                future_to_video = {executor.submit(process_video_worker, video_info): video_info for video_info in video_paths}
                
                # ä½¿ç”¨tqdmé¡¯ç¤ºé€²åº¦
                for future in tqdm(concurrent.futures.as_completed(future_to_video), 
                                 total=len(video_paths), desc=f"Processing {dataset_name}"):
                    video_info = future_to_video[future]
                    try:
                        face_count = future.result()
                    except Exception as exc:
                        logging.error(f"Video {video_info.get('video_path', 'unknown')} generated an exception: {exc}")
                        with stats_lock:
                            stats["errors"] += 1
            
            logging.info(f"Completed {dataset_name}: {stats['processed']} videos, {stats['total_faces']} faces, {stats['errors']} errors")
            return stats
            
        except Exception as e:
            logging.error(f"Error processing dataset {dataset_name}: {e}")
            return {"processed": 0, "total_faces": 0, "errors": 1}
    
    def generate_manifests(self):
        """ç”Ÿæˆæ•°æ®æ¸…å•æ–‡ä»¶"""
        processed_path = self.path_config.config["base_paths"]["processed_data"]
        manifests_dir = f"{processed_path}/manifests"
        
        # ä¸ºæ¯ä¸ªæ•°æ®é›†åˆ’åˆ†ç”Ÿæˆæ¸…å•
        splits_to_process = [
            ('train', f"{processed_path}/train"),
            ('val', f"{processed_path}/val"),
            ('test_celebdf_v2', f"{processed_path}/final_test_sets/celebdf_v2"),
            ('test_ffpp', f"{processed_path}/final_test_sets/ffpp"),
            ('test_dfdc', f"{processed_path}/final_test_sets/dfdc")
        ]
        
        for split_name, split_path in splits_to_process:
            if not os.path.exists(split_path):
                continue
            
            manifest_data = []
            
            # å¤„ç†æœ‰real/fakeå­ç›®å½•çš„æƒ…å†µ
            if split_name.startswith('test_') and split_name != 'test_dfdc':
                for label in ['real', 'fake']:
                    label_path = os.path.join(split_path, label)
                    
                    if not os.path.exists(label_path):
                        continue
                    
                    for image_file in os.listdir(label_path):
                        if image_file.endswith('.png'):
                            relative_path = os.path.join(split_name.replace('test_', ''), label, image_file)
                            label_numeric = 0 if label == 'real' else 1
                            
                            manifest_data.append({
                                'filepath': relative_path,
                                'label': label_numeric,
                                'label_name': label,
                                'dataset': split_name.replace('test_', '')
                            })
            
            # å¤„ç†DFDCç­‰æ··åˆç›®å½•çš„æƒ…å†µ
            elif split_name == 'test_dfdc':
                for image_file in os.listdir(split_path):
                    if image_file.endswith('.png'):
                        # ä»æ–‡ä»¶åæ¨æ–­æ ‡ç­¾ (éœ€è¦åœ¨å¤„ç†æ—¶ä¿å­˜è¿™ä¸ªä¿¡æ¯)
                        relative_path = os.path.join('dfdc', image_file)
                        
                        manifest_data.append({
                            'filepath': relative_path,
                            'label': -1,  # éœ€è¦åç»­å¤„ç†
                            'label_name': 'unknown',
                            'dataset': 'dfdc'
                        })
            
            # å¤„ç†train/valç›®å½•
            else:
                for label in ['real', 'fake']:
                    label_path = os.path.join(split_path, label)
                    
                    if not os.path.exists(label_path):
                        continue
                    
                    for image_file in os.listdir(label_path):
                        if image_file.endswith('.png'):
                            relative_path = os.path.join(split_name, label, image_file)
                            label_numeric = 0 if label == 'real' else 1
                            
                            manifest_data.append({
                                'filepath': relative_path,
                                'label': label_numeric,
                                'label_name': label,
                                'dataset': 'mixed'
                            })
            
            # ä¿å­˜æ¸…å•æ–‡ä»¶
            if manifest_data:
                manifest_df = pd.DataFrame(manifest_data)
                manifest_file = os.path.join(manifests_dir, f"{split_name}_manifest.csv")
                manifest_df.to_csv(manifest_file, index=False)
                logging.info(f"Generated manifest: {manifest_file} with {len(manifest_data)} samples")
    
    def run_full_preprocessing(self, datasets: Optional[List[str]] = None):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµç¨‹"""
        logging.info("Starting full dataset preprocessing...")
        logging.info(f"Target image format: {self.processing_config.get('image_size', [256, 256])} PNG (matching DF40 specifications)")
        
        # éªŒè¯é…ç½®è·¯å¾„
        validation_results = self.path_config.validate_paths()
        for path_name, exists in validation_results.items():
            status = "OK" if exists else "ERROR"
            logging.info(f"{status} {path_name}")
        
        # ç¡®å®šè¦å¤„ç†çš„æ•°æ®é›† (è¿‡æ»¤æ‰DF40)
        if datasets is None:
            datasets = [name for name in self.path_config.config["datasets"].keys() if name != "df40"]
        else:
            datasets = [name for name in datasets if name != "df40"]
        
        logging.info(f"Processing datasets: {datasets} (DF40 skipped - already preprocessed)")
        
        total_stats = {"processed": 0, "total_faces": 0, "errors": 0}
        
        # é€ä¸ªå¤„ç†æ•°æ®é›†
        for dataset_name in datasets:
            if dataset_name in self.path_config.config["datasets"]:
                stats = self.process_dataset(dataset_name)
                for key in total_stats:
                    if key in stats:
                        total_stats[key] += stats[key]
            else:
                logging.warning(f"Dataset {dataset_name} not found in configuration")
        
        logging.info(f"All datasets processed: {total_stats['processed']} videos, {total_stats['total_faces']} faces, {total_stats['errors']} errors")
        
        # ç”Ÿæˆæ¸…å•æ–‡ä»¶
        self.generate_manifests()
        logging.info("All manifest files generated successfully")
        
        return total_stats

# =============================================================================
# ä¸»ç¨‹åºå…¥å£
# =============================================================================

def check_gpu_requirements():
    """æ£€æŸ¥GPUè¦æ±‚"""
    print("=== GPUè¦æ±‚æ£€æŸ¥ ===")
    
    try:
        import torch
        if not torch.cuda.is_available():
            print("âŒ é”™è¯¯ï¼šæœªæ£€æµ‹åˆ°CUDAæ”¯æŒçš„PyTorch")
            print("è¯·å®‰è£…æ”¯æŒCUDAçš„PyTorch:")
            print("conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia")
            return False
        
        gpu_count = torch.cuda.device_count()
        current_gpu = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_gpu)
        
        print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUè®¾å¤‡")
        print(f"âœ… å½“å‰ä½¿ç”¨GPU: {gpu_name} (è®¾å¤‡ID: {current_gpu})")
        
        # æ£€æŸ¥GPUå†…å­˜
        total_memory = torch.cuda.get_device_properties(current_gpu).total_memory / (1024**3)
        print(f"âœ… GPUå†…å­˜: {total_memory:.2f}GB")
        
        if total_memory < 4.0:
            print("âš ï¸  è­¦å‘Šï¼šGPUå†…å­˜å¯èƒ½ä¸è¶³ï¼ˆå»ºè®®è‡³å°‘4GBï¼‰")
        
        # æ£€æŸ¥OpenCV CUDAæ”¯æŒ (å®‰å…¨æ£€æŸ¥)
        try:
            cuda_devices = cv2.cuda.getCudaEnabledDeviceCount()
            if cuda_devices > 0:
                print(f"âœ… OpenCV CUDAæ”¯æŒ: {cuda_devices} ä¸ªè®¾å¤‡ - è§†é¢‘å¤„ç†å°†ä½¿ç”¨GPUåŠ é€Ÿ")
                # æµ‹è¯•OpenCV CUDAåŠŸèƒ½
                try:
                    test_mat = cv2.cuda_GpuMat()
                    print("âœ… OpenCV GPUå†…å­˜åˆ†é…æµ‹è¯•é€šè¿‡")
                except Exception as e:
                    print(f"âš ï¸  OpenCV CUDAåˆå§‹åŒ–é—®é¢˜: {e}")
            else:
                print("âš ï¸  OpenCVæ²¡æœ‰CUDAæ”¯æŒ - è§†é¢‘å¤„ç†å°†ä½¿ç”¨CPU")
                print("   (äººè„¸æ£€æµ‹ä»å°†ä½¿ç”¨GPUåŠ é€Ÿ)")
        except AttributeError:
            print("âš ï¸  OpenCVç‰ˆæœ¬æ²¡æœ‰CUDAæ¨¡å— - è§†é¢‘å¤„ç†å°†ä½¿ç”¨CPU")
            print("   (è¿™ä¸å½±å“ä¸»è¦çš„GPUåŠ é€ŸåŠŸèƒ½ï¼šäººè„¸æ£€æµ‹)")
            print("   å¯é¸å®‰è£CUDAç‰ˆæœ¬: conda install opencv-cuda -c conda-forge")
        
        # æª¢æŸ¥è¦–é »è™•ç†å¾Œç«¯
        print("\n=== è¦–é »è™•ç†å¾Œç«¯æª¢æŸ¥ ===")
        available_backends = []
        for backend, available in VIDEO_BACKENDS.items():
            if available:
                available_backends.append(backend)
                if backend == 'opencv':
                    print(f"âœ… {backend.upper()}: å¯ç”¨ (CPUè¦–é »è™•ç†)")
                else:
                    print(f"âœ… {backend.upper()}: å¯ç”¨ (GPUè¦–é »è™•ç†)")
            else:
                print(f"âŒ {backend.upper()}: ä¸å¯ç”¨")
        
        if len(available_backends) > 1:
            # Windowså¹³å°æ¨è–¦é †åº
            windows_priority = ['decord', 'torchvision', 'dali']
            best_backend = next((b for b in windows_priority if b in available_backends), 'opencv')
            print(f"ğŸš€ Windowsæ¨è–¦ä½¿ç”¨: {best_backend.upper()}")
            
            if 'dali' in available_backends:
                print("âš ï¸ æ³¨æ„ï¼šDALIåœ¨Windowsä¸Šå¯èƒ½æœ‰å…¼å®¹æ€§å•é¡Œï¼Œå»ºè­°ä½¿ç”¨Decord")
        
        print("\nâœ… GPUæ£€æŸ¥é€šè¿‡ - å¯ä»¥å¼€å§‹GPUåŠ é€Ÿé¢„å¤„ç†")
        print("")
        return True
        
    except ImportError:
        print("âŒ é”™è¯¯ï¼šPyTorchæœªå®‰è£…")
        print("è¯·å®‰è£…PyTorch: conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia")
        return False
    except Exception as e:
        print(f"âŒ GPUæ£€æŸ¥å¤±è´¥: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="Dataset Preprocessing V2 for Deepfake Detection - REQUIRES GPU")
    parser.add_argument("--config", default="config/dataset_paths.json", help="Path to configuration file")
    parser.add_argument("--datasets", nargs="+", help="Specific datasets to process (celebdf_v2, ffpp, dfdc)")
    parser.add_argument("--validate-only", action="store_true", help="Only validate paths without processing")
    parser.add_argument("--print-config", action="store_true", help="Print configuration summary and exit")
    parser.add_argument("--skip-gpu-check", action="store_true", help="Skip GPU requirement check (NOT RECOMMENDED)")
    parser.add_argument("--video-backend", choices=['auto', 'torchvision', 'decord', 'dali', 'opencv'], 
                        default='auto', help="Choose video processing backend for GPU acceleration")
    parser.add_argument("--face-detector", choices=['auto', 'yolov8', 'mediapipe', 'insightface', 'opencv_dnn', 'facenet_pytorch', 'mtcnn'], 
                        default='auto', help="Choose face detection backend for GPU acceleration")
    parser.add_argument("--workers", type=int, default=0, 
                        help="Number of parallel workers (0=auto, max 4 to balance GPU usage)")
    
    args = parser.parse_args()
    
    print("=== AWARE-NET Dataset Preprocessing V2 (GPU Required) ===")
    print("Targeting DF40-compatible format: 256x256 PNG images")
    print("Processing: CelebDF-V2, FF++, DFDC (DF40 skipped - already preprocessed)")
    print("")
    
    # GPUè¦æ±‚æ£€æŸ¥
    if not args.skip_gpu_check:
        if not check_gpu_requirements():
            print("GPUæ£€æŸ¥å¤±è´¥ï¼é¢„å¤„ç†éœ€è¦GPUæ”¯æŒã€‚")
            print("å¦‚æœè¦å¼ºåˆ¶è·³è¿‡æ­¤æ£€æŸ¥ï¼Œè¯·ä½¿ç”¨ --skip-gpu-check å‚æ•°")
            return 1
    
    # åˆ›å»ºé¢„å¤„ç†å™¨
    try:
        preprocessor = DatasetPreprocessorV2(
            config_file=args.config,
            video_backend=args.video_backend,
            face_detector=args.face_detector
        )
        preprocessor.set_workers(args.workers)
    except Exception as e:
        print(f"Error initializing preprocessor: {e}")
        return 1
    
    # æ‰“å°é…ç½®æ‘˜è¦
    if args.print_config:
        preprocessor.path_config.print_config_summary()
        return 0
    
    # éªŒè¯è·¯å¾„
    if args.validate_only:
        validation_results = preprocessor.path_config.validate_paths()
        print("Path validation results:")
        for path_name, exists in validation_results.items():
            status = "OK" if exists else "ERROR"
            print(f"  {status} {path_name}")
        return 0
    
    # è¿è¡Œé¢„å¤„ç†
    try:
        stats = preprocessor.run_full_preprocessing(args.datasets)
        print(f"\n=== Preprocessing completed successfully! ===")
        print(f"Total videos processed: {stats['processed']}")
        print(f"Total faces extracted: {stats['total_faces']}")
        print(f"Errors encountered: {stats['errors']}")
        print(f"Output format: 256x256 PNG images (DF40-compatible)")
        print(f"Output directory: {preprocessor.path_config.config['base_paths']['processed_data']}")
        return 0
    except Exception as e:
        logging.error(f"Preprocessing failed: {e}")
        return 1

if __name__ == "__main__":
    exit(main())