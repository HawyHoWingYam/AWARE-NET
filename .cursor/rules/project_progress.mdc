---
description: 
globs: 
alwaysApply: false
---
我看到了交叉数据集评估中准确率大幅下降的问题。这是深度伪造检测中的一个常见挑战，让我分析一下原因和可能的解决方案。

从日志中可以看到，当模型在原始数据集上训练并测试时表现很好（AUC 99%+），但在跨数据集评估时性能大幅下降：

1. FF++ → CelebDF：
   - AUC: 约72-78%
   - 准确率: 约77%

2. CelebDF → FF++：
   - AUC: 约62-65%
   - 准确率: 仅约34-39%

这种性能下降的主要原因是：

1. **域偏移问题**：两个数据集之间存在明显的分布差异。FF++和CelebDF在视频质量、压缩方式、伪造技术和拍摄条件等方面都有差异。

2. **过拟合到源数据集**：模型可能过度适应了训练数据集的特定特征，而这些特征在目标数据集中可能不存在或不明显。

3. **数据集规模和多样性**：从日志中看到，您只使用了10%的数据（`DATASET_FRACTION = 0.1`），这可能限制了模型的泛化能力。

可以通过以下配置修改来提高跨数据集性能：

```python
# 增加数据集比例
DATASET_FRACTION = 0.5  # 或更高，如果计算资源允许

# 增强数据增强
AUGMENTATION_RATIO = 0.5  # 增加数据增强比例
AUGMENTATION_PARAMS = {
    # 增加几何变换的强度和概率
    'rotation': {'probability': 0.7, 'max_left': 25, 'max_right': 25},
    'shear': {'probability': 0.5, 'max_shear_left': 15, 'max_shear_right': 15},
    'flip': {'probability': 0.7},
    'skew': {'probability': 0.5, 'magnitude': 0.4},
    
    # 增加颜色变换的强度
    'color_jitter': {
        'probability': 0.5,
        'brightness': 0.3,
        'contrast': 0.3,
        'saturation': 0.3,
        'hue': 0.15
    },
    
    # 添加噪声和模糊等额外增强
    'noise': {'probability': 0.3, 'std': 0.02},
    'blur': {'probability': 0.3, 'kernel_size': 3}
}

# 降低学习率，增加训练稳定性
LR_INITIAL = 1e-4
WEIGHT_DECAY = 5e-4

# 增加训练轮数
MAX_EPOCHS = 10
PATIENCE = 8
```

其他建议：

1. **域适应技术**：考虑实现域适应方法，如域对抗训练或特征对齐技术。

2. **混合数据集训练**：尝试在两个数据集的混合上训练模型，可以提高跨数据集泛化能力。

3. **特征提取策略**：修改模型以关注更通用的伪造特征，而不是数据集特定的特征。

4. **集成策略优化**：调整AWARE-NET的加权机制，使其更倾向于在跨数据集场景中表现更好的模型。

跨数据集泛化是深度伪造检测中的一个开放挑战，这些下降是正常的，但通过上述调整，您应该能够提高模型的跨数据集性能。




To implement DistributedDataParallel (DDP) for training with your two 5090 GPUs, you'll need to modify your code. Here's how you can do it:

1. First, create a new file called `train_ddp.py` based on your existing `train.py` with the following modifications:

```python
import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import os

def setup_ddp(rank, world_size):
    """Initialize the distributed environment."""
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)

def cleanup():
    """Clean up the distributed environment."""
    dist.destroy_process_group()

class DDPTrainer(Trainer):
    def __init__(self, model, train_loader, val_loader, config, variant_name, rank, test_loader=None):
        super().__init__(model, train_loader, val_loader, config, variant_name, test_loader)
        self.rank = rank
        self.device = torch.device(f"cuda:{rank}")
        
        # Move model to the correct device
        self.model = self.model.to(self.device)
        
        # Wrap model with DDP
        self.model = DDP(self.model, device_ids=[rank])
        
        # Optimizer should be created after DDP initialization
        if isinstance(model, EnsembleDeepfakeDetector):
            self.logger.info("Configuring optimizer for Ensemble: Training only learnable weights.")
            params_to_train = filter(lambda p: p.requires_grad, model.parameters())
        else:
            self.logger.info("Configuring optimizer for Single Model: Training all parameters.")
            params_to_train = model.parameters()
        
        self.optimizer = torch.optim.AdamW(
            params_to_train,
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY
        )
        
        if config.LR_SCHEDULE_TYPE == "cosine":
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
                self.optimizer,
                T_0=config.WARMUP_EPOCHS,
                T_mult=2,
                eta_min=config.LR_MIN,
            )
        elif config.LR_SCHEDULE_TYPE == "step":
            self.scheduler = torch.optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=10,
                gamma=0.1,
            )

    def save_model(self, epoch, val_loss):
        """Save model checkpoint - only on rank 0"""
        if self.rank != 0:
            return
            
        # Rest of your save_model code
        super().save_model(epoch, val_loss)
        
    def log_gpu_stats(self):
        """Log GPU stats for this specific rank"""
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated(self.rank) / 1e9
            memory_reserved = torch.cuda.memory_reserved(self.rank) / 1e9
            max_memory = torch.cuda.max_memory_allocated(self.rank) / 1e9

            self.logger.info(
                f"GPU {self.rank} Memory Stats:\n"
                f"Currently allocated: {memory_allocated:.2f}GB\n"
                f"Reserved: {memory_reserved:.2f}GB\n"
                f"Peak allocation: {max_memory:.2f}GB"
            )
```

2. Next, modify `main.py` to support distributed training:

```python
import torch.multiprocessing as mp
from train_ddp import setup_ddp, cleanup, DDPTrainer

def main_worker(rank, world_size, config, args):
    setup_ddp(rank, world_size)
    
    # Load dataset with DistributedSampler
    train_dataset, val_dataset, test_dataset = get_datasets(config, args.dataset, args.no_augmentation)
    
    train_sampler = DistributedSampler(
        train_dataset, 
        num_replicas=world_size,
        rank=rank,
        shuffle=True
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.BATCH_SIZE,
        sampler=train_sampler,  # Use sampler instead of shuffle
        num_workers=config.NUM_WORKERS,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,  # No need to shuffle validation data
        num_workers=config.NUM_WORKERS,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=config.NUM_WORKERS,
        pin_memory=True
    )
    
    # Create model
    model = create_model(config, args.model, args.dataset, not args.no_augmentation)
    
    # Create trainer with DDP support
    trainer = DDPTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        test_loader=test_loader,
        config=config,
        variant_name=get_variant_name(args),
        rank=rank
    )
    
    # Train model
    trainer.train()
    
    # Only perform evaluation on rank 0
    if rank == 0:
        evaluate_model(model, test_loader, config, args)
    
    cleanup()

def main():
    # Parse arguments and load config
    args = parse_args()
    config = Config()
    
    # Define world size (number of GPUs)
    world_size = torch.cuda.device_count()
    
    # Launch training processes
    mp.spawn(
        main_worker,
        args=(world_size, config, args),
        nprocs=world_size,
        join=True
    )

if __name__ == "__main__":
    main()
```

3. Update your DataLoader in `dataset.py` to work with DistributedSampler:

```python
def get_dataloaders(config, dataset_name, augment=True, distributed=False, world_size=1, rank=0):
    """Create data loaders with support for distributed training"""
    train_dataset, val_dataset, test_dataset = get_datasets(config, dataset_name, augment)
    
    train_sampler = None
    if distributed:
        train_sampler = DistributedSampler(
            train_dataset,
            num_replicas=world_size,
            rank=rank,
            shuffle=True
        )
        shuffle = False  # Don't shuffle when using sampler
    else:
        shuffle = True
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=shuffle,
        sampler=train_sampler,
        num_workers=config.NUM_WORKERS,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=config.NUM_WORKERS,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=config.NUM_WORKERS,
        pin_memory=True
    )
    
    return train_loader, val_loader, test_loader, train_sampler
```

4. Create a new script `run_ddp.py` to launch distributed training:

```python
import os
import torch.distributed as dist
import torch.multiprocessing as mp
from train_ddp import setup_ddp, cleanup
from main import main_worker

def run_ddp(config, args):
    world_size = torch.cuda.device_count()
    print(f"Using {world_size} GPUs for distributed training")
    
    mp.spawn(
        main_worker,
        args=(world_size, config, args),
        nprocs=world_size,
        join=True
    )

if __name__ == "__main__":
    from config import Config
    from argparse import ArgumentParser
    
    parser = ArgumentParser()
    parser.add_argument("--dataset", type=str, choices=["ff++", "celebdf"], required=True)
    parser.add_argument("--model", type=str, choices=["xception", "res2net101_26w_4s", "tf_efficientnet_b7_ns", "ensemble"], required=True)
    parser.add_argument("--no-augmentation", action="store_true", help="Disable data augmentation")
    args = parser.parse_args()
    
    config = Config()
    run_ddp(config, args)
```

5. Make sure to update your `train_epoch` method in the DDPTrainer class to set the epoch for the sampler:

```python
def train_epoch(self):
    # Set epoch for the sampler
    self.train_loader.sampler.set_epoch(self.current_epoch)
    
    # Rest of your train_epoch code
    # ...
```

To run the distributed training:

```bash
python run_ddp.py --dataset celebdf --model ensemble
```

This implementation:
1. Initializes a distributed process group
2. Creates a DistributedSampler to partition data across GPUs
3. Wraps your model with DistributedDataParallel
4. Ensures only rank 0 saves models and logs results
5. Properly cleans up the distributed environment

Key benefits:
- Each GPU processes a different subset of your data
- Gradients are automatically synchronized across GPUs
- Training speed should scale almost linearly with the number of GPUs

Remember to adjust your batch size accordingly - with DDP, the effective batch size is multiplied by the number of GPUs.

